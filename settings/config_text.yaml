# =====================================================================
# settings/config_text.yaml (single source of truth, paper-strict)
# - NO python tuple tags
# - ngram_range kept as list [a,b], code converts to tuple
# =====================================================================

common:
  seed: &seed 42

  split_test_size: &split_test_size 0.2
  length_std_k: &length_std_k 2.5
  min_per_class: &min_per_class 2

  logreg_C: &logreg_C 1.0
  logreg_max_iter: &logreg_max_iter 1000
  max_seq_length: &max_seq_length 512

asr:
  # choose ONE:
  data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/raw_vad"
  # data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/raw"
  # data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/merge"
  # data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/merge_vad"

  output_csv: "/content/ncmmsc_merged_asr_transcripts.csv"
  model_size: "large-v3"
  device: "cuda"
  compute_type: "float16"

  initial_prompt: |-
    請以繁體中文逐字轉寫（不要翻譯/潤飾/摘要）。
    保留語助詞與猶豫聲、重複、自我修正與中斷。
    聽不清楚以【聽不清楚】標示；AD/HC/MCI 保留大寫。
    常見詞：失智症、阿茲海默症、認知功能、記憶力、量表、測驗、醫師、受試者、照顧者。

  decode:
    language: "zh"
    task: "transcribe"
    beam_size: 10
    patience: 1.0
    length_penalty: 1.0
    temperature: 0.0
    compression_ratio_threshold: 2.4
    log_prob_threshold: -1.0
    no_speech_threshold: 0.6
    condition_on_previous_text: false
    vad_filter: true
    without_timestamps: true

predictive:
  meta_csv: "/content/NCMMSC2021_AD_Competition-dev/2_final_list_train.csv"
  egemaps_csv: "/content/NCMMSC2021_AD_Competition-dev/egemaps_final.csv"
  tsv_root: "/content/NCMMSC2021_AD_Competition-dev/tsv2"
  output_text_jsonl: &predictive_jsonl "/content/Chinese-predictive_challenge_tsv2_output.jsonl"
  output_egemaps_csv: "/content/predictive_egemaps_features.csv"
  dataset_name: "Chinese_predictive_challenge"

  keep_speakers: &pred_keep_speakers ["<B>"]
  tsv:
    keep_speakers: *pred_keep_speakers
    drop_silence: true
    order_by: "no"

text:
  output_dir: "/content/chinese_combined"
  combined_name: "Chinese_Combined.jsonl"
  cleaned_jsonl: "/content/chinese_combined/cleaned.jsonl"

  ncmmsc_jsonl: &ncmmsc_jsonl "/content/chinese_combined/ncmmsc_from_asr.jsonl"

  corpora:
    - name: "NCMMSC2021_AD_Competition"
      path: *ncmmsc_jsonl
    - name: "Chinese_predictive_challenge"
      path: *predictive_jsonl
    - name: "TAUKADIAL"
      path: null

  # 合併」（merge）兩個 text corpora 再一起清理/抽樣/評估
  target_datasets:
    - "NCMMSC2021_AD_Competition"
    - "Chinese_predictive_challenge"

  target_labels: ["AD", "HC"]

  balance: true
  subset_seed: *seed
  cap_per_class: 300

  # label merge：對齊文獻（Probable/Possible -> AD；Control -> HC）
  label_map:
    # ---- HC / Control variants ----
    NC: "HC"
    CN: "HC"
    CTRL: "HC"
    CONTROL: "HC"
    CONTROLS: "HC"
    HEALTHY: "HC"
    NORMAL: "HC"
    "Control": "HC"
    "Controls": "HC"
    "Healthy": "HC"
    "Normal": "HC"
    "HEALTHYCONTROL": "HC"
    "HEALTHY_CONTROL": "HC"
    "HEALTHY CONTROL": "HC"

    # ---- AD variants ----
    PROBABLEAD: "AD"
    POSSIBLEAD: "AD"
    "ProbableAD": "AD"
    "PossibleAD": "AD"
    "PROBABLE_AD": "AD"
    "POSSIBLE_AD": "AD"
    "PROBABLE AD": "AD"
    "POSSIBLE AD": "AD"

    # ---- MCI ----
    MILDCOGNITIVEIMPAIRMENT: "MCI"
    "MildCognitiveImpairment": "MCI"
    "MILD_COGNITIVE_IMPAIRMENT": "MCI"
    "MILD COGNITIVE IMPAIRMENT": "MCI"
    MCI: "MCI"

  language_filter:
    drop_languages: ["en"]

  # prompt filter（解決ASR 混講者用；不影響 tsv 已分離講者的文本）
  prompt_filter:
    enabled: true
    apply_datasets: ["NCMMSC2021_AD_Competition"]

    # Pass1: leading（只掃開頭，避免誤殺受試者內容）
    mode: "leading"
    max_leading_sentences: 12
    min_keep_chars: 20

    patterns:
      # ===== A) 訪談者「任務指令」=====
      - "^[\\s　]*(好|那|那我們|那我们|現在|现在|接下來|接下来|請|麻煩|麻烦).{0,10}(你|您).{0,10}(描述|說|说|講|讲|告訴我|告诉我)"
      - "^[\\s　]*(請|麻煩|麻烦).{0,12}(把|再把|盡量|尽量).{0,10}(你|您)?.{0,6}(看到|看見|看见).{0,12}(告訴我|告诉我|說|说|講|讲|描述)"
      - "^[\\s　]*(我們|咱們|咱们).{0,12}(來看|来看|看一下|看一看).{0,12}(這張|这张|這個|这个).{0,4}(圖|图|圖片|图片|畫|画)"
      - "^[\\s　]*(然後呢|然后呢|還有呢|还有呢)[?？]?$"

      # ===== B) 訪談者「引導追問」=====
      - "^[\\s　]*(你|您).{0,10}(看到|看見|看见).{0,12}(什麼|什么)[?？]?$"
      - "^[\\s　]*(他們|他们).{0,12}(在做什麼|在做什么)[?？]?$"
      - "^[\\s　]*(越多越好|說得越多越好|说得越多越好|講得越多越好|讲得越多越好)$"

    # Pass2: echo any
    echo_enabled: true
    echo_patterns:
      # ===== C) initial_prompt 回音 / 規則語句 =====
      - "(若有單字實在聽不清楚|请不要乱猜|請不要亂猜)"
      - "(請以繁體中文|请以繁体中文|完整逐字轉寫|完整逐字转写)"
      - "(不要翻譯|不要翻译|不要潤飾|不要润饰|不要做任何摘要)"
      - "(不要自行更改語序|不要自行更改语序|補上沒聽到的字|补上没听到的字)"
      - "(請保留口語語氣詞|请保留口语语气词|這些特徵可能與認知功能相關|这些特征可能与认知功能相关)"
      - "(遇到下列專有名詞|遇到下列专有名词|對於\\s*AD、?\\s*MCI、?\\s*HC|对于\\s*AD、?\\s*MCI、?\\s*HC)"

      # ===== D) 訪談者 meta 指令 =====
      - "^[\\s　]*(這|这)(個|句|也).{0,6}(也)?(要)?(說|说|講|讲)(嗎|吗)[?？]?$"
      - "^[\\s　]*(這|这).{0,6}(要不要|要不要再).{0,6}(說|说|講|讲)(一下|一點|一点|點)?[?？]?$"
      - "^[\\s　]*(還|还)?(要不要|要不要再|還要|还要).{0,6}(說|说|講|讲)(一下|一點|一点|點)?[?？]?$"
      - "^[\\s　]*(那|那麼|那么).{0,6}(這|这)(個|句).{0,6}(也)?(要)?(說|说|講|讲)(嗎|吗)[?？]?$"

  # quality filter (AFTER cleaning + prompt_filter)
  quality_filter:
    enabled: true
    min_chars: 15
    min_lex_chars: 8
    min_han: 6
    max_unk_ratio: 0.75

  length_filter:
    enabled: true
    std_k: *length_std_k
    min_class_n: 30

features:
  tfidf:
    vectorizer:
      analyzer: "char"
      ngram_range: [2, 4]
      lowercase: false
      max_features: 50000
      stop_words: null
      min_df: 1
      max_df: 1.0
    transformer:
      use_idf: true
      smooth_idf: true
      norm: "l2"
      sublinear_tf: false
    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  bert:
    model_name: "/content/models/bert-base-chinese"
    embedding_strategy: "mean"
    pooling: "mean"
    last_n_layers: 4
    max_seq_length: *max_seq_length
    device: "cuda"
    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  glove:
    embeddings_path: "/content/embeddings/cc.zh.300.vec"
    embedding_dim: 300
    tokenizer: "jieba"
    max_words: 50000
    lowercase: false
    remove_stopwords: false
    stopwords_lang: null
    pooling: "sum_l2norm"
    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  gemma:
    model_name: "/content/models/google__gemma-2b"
    pooling: "mean"
    max_seq_length: *max_seq_length
    device: "cuda"
    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  crossval:
    enabled: true
    n_splits: 5
    shuffle: true
    random_state: *seed
    output_indices: "/content/chinese_combined/folds_indices.json"

    methods: ["tfidf", "bert", "glove", "gemma"]

    tfidf_fit_scope: "train"   # "full" | "train"

    batch_size: 8
    bert_batch_size: 8
    gemma_batch_size: 4

    metrics_average: "macro"
    zero_division: 0
    metrics_output: "/content/chinese_combined/cv_metrics.json"

    print_classification_report: true
    print_confusion_matrix: true
    print_method_summary: true
