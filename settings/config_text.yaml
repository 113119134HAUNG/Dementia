# =====================================================================
# config_text.yaml
#
# Central configuration for:
#   1. ASR on NCMMSC audio (Whisper)
#   2. Predictive dataset preprocessing (TSV + eGeMAPS)
#   3. Chinese text preprocessing & dataset merging
#   4. Text features + classifiers + K-fold CV
#      (TF-IDF / BERT / GloVe / Gemma)
# =====================================================================

# ---------------------------------------------------------------------
# Common shared values (single source of truth; reuse via YAML anchors)
# ---------------------------------------------------------------------
common:
  seed: &seed 42

  # preprocessing
  split_test_size: &split_test_size 0.2
  length_std_k: &length_std_k 1.0
  min_per_class: &min_per_class 2

  # modeling (logreg / seq length)
  logreg_C: &logreg_C 1.0
  logreg_max_iter: &logreg_max_iter 1000
  max_seq_length: &max_seq_length 512

# ---------------------------------------------------------------------
#   ASR (audio → transcript CSV) – used by asr_ncmmsc.py
# ---------------------------------------------------------------------
asr:
  # Root directory of NCMMSC audio (contains AD / HC / MCI subfolders)
  data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/merge"

  # Output ASR CSV (used later by text preprocessing)
  output_csv: "/content/ncmmsc_merged_asr_transcripts.csv"

  # Whisper model settings
  model_size: "large-v2"
  device: "cuda"            # "cuda" or "cpu"
  compute_type: "float16"   # e.g. "float16", "int8_float16", ...

  # Initial prompt for Whisper (Chinese clinical context)
  initial_prompt: |-
    以下是一段中文口語錄音的逐字稿，內容為針對失智症、阿茲海默症、
    輕度認知障礙（MCI）與健康對照組（HC）的臨床訪談與語言測驗，
    包括圖畫描述任務、語意流暢度測驗以及日常生活相關問答。
    請以繁體中文完整逐字轉寫受試者與訪談者的口語內容，不要翻譯、
    不要潤飾或做任何摘要，也不要自行更改語序或補上沒聽到的字。
    請保留口語語氣詞與猶豫聲（例如：嗯、呃、啊、就是、然後）、
    重複、修正、語法不完整或中斷的句子，這些特徵可能與認知功能相關。
    遇到下列專有名詞或相關用語時，請盡量使用常見且一致的寫法：
    「失智症」、「阿茲海默症」、「阿茲海默病」、「輕度認知障礙」、
    「MCI」、「AD」、「HC」、「記憶力」、「認知功能」、
    「量表」、「測驗」、「醫師」、「受試者」、「照顧者」。
    對於 AD、MCI、HC 等縮寫，請以大寫英文字母保留。
    若有單字實在聽不清楚，請不要亂猜，可以以『[聽不清楚]』標示。

  # Decoding hyper-parameters for Whisper (all model-side settings live here)
  decode:
    language: "zh"
    task: "transcribe"
    beam_size: 10
    patience: 1.0
    length_penalty: 1.0
    temperature: 0.0
    compression_ratio_threshold: 2.4
    log_prob_threshold: -1.0
    no_speech_threshold: 0.6
    condition_on_previous_text: true
    vad_filter: false
    without_timestamps: true

# ---------------------------------------------------------------------
#    Predictive dataset (TSV + eGeMAPS) – used by preprocess_predictive.py
# ---------------------------------------------------------------------
predictive:
  # Original meta file with labels and demographics
  # (e.g., uuid, label, sex, age, education)
  meta_csv: "/content/predictive/2_final_list_train.csv"

  # eGeMAPS feature table (one row per uuid)
  egemaps_csv: "/content/predictive/egemaps_final.csv"

  # Directory containing per-utterance TSV transcripts (Pxxxx_xxxx.tsv)
  tsv_root: "/content/predictive/tsv"

  # Output text JSONL (will later be used by `text.predictive_jsonl`)
  output_text_jsonl: "/content/Chinese-predictive_challenge_tsv2_output.jsonl"

  # Output eGeMAPS feature CSV for acoustic experiments
  output_egemaps_csv: "/content/predictive_egemaps_features.csv"

# ---------------------------------------------------------------------
#    Text preprocessing & dataset merging – used by preprocess_chinese.py
# ---------------------------------------------------------------------
text:
  # JSONL converted from ASR CSV for NCMMSC
  ncmmsc_jsonl: "/content/data_Chinese-NCMMSC2021_AD_Competition.jsonl"

  # Chinese predictive dataset in JSONL form
  # (produced by preprocess_predictive.py → predictive.output_text_jsonl)
  predictive_jsonl: "/content/Chinese-predictive_challenge_tsv2_output.jsonl"

  # (Optional) TAUKADIAL JSONL – can be omitted if not available
  # taukadial_jsonl: "/content/TAUKADIAL.jsonl"

  # Directory to store merged & preprocessed JSONL files
  output_dir: "/content/chinese_combined"

  # Name of merged JSONL combining all Chinese datasets
  combined_name: "Chinese_NCMMSC_iFlyTek.jsonl"

  # Filenames for final train/test splits (written into output_dir)
  train_jsonl: "train_chinese.jsonl"
  test_jsonl: "test_chinese.jsonl"

  # subset rules (driven by YAML; code should not hardcode any defaults)
  target_datasets: ["NCMMSC2021_AD_Competition"]   # 只用 NCMMSC
  target_labels: ["AD", "HC"]                      # binary：AD vs HC
  balance: true                                    # 類別平衡（抽到同數）
  subset_seed: *seed
  cap_per_class: 300                               # debug 快速跑

  # length-based outlier filter
  length_filter:
    std_k: *length_std_k                           # keep within mean ± std_k * std

  # train/test split settings
  split:
    test_size: *split_test_size
    random_state: *seed
    min_per_class: *min_per_class                  # each class must have >= this count

# ---------------------------------------------------------------------
#    Text features + classifiers (TF-IDF / BERT / GloVe / Gemma)
#    + K-fold cross-validation
# ---------------------------------------------------------------------
features:
  # Which text feature method to run in the current experiment:
  #   "tfidf" / "bert" / "glove" / "gemma"
  method: "bert"

  # ---------------- TF-IDF + Logistic Regression --------------------
  tfidf:
    vectorizer:
      lowercase: true
      max_features: 20000
      ngram_range: [1, 2]      # (1, 2) = unigram + bigram
      stop_words: null         # Chinese: keep null, or set your own list
      min_df: 1
      max_df: 1.0

    transformer:
      use_idf: true
      smooth_idf: true
      norm: "l2"
      sublinear_tf: false

    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  # ---------------- BERT embeddings + Logistic Regression -----------
  bert:
    # For Chinese experiments, use a Chinese or multilingual model
    # e.g. "bert-base-chinese" or "hfl/chinese-roberta-wwm-ext"
    model_name: "bert-base-chinese"
    pooling: "mean"            # "mean" / "cls" / "last4_mean"
    max_seq_length: *max_seq_length

    device: "cuda"             # or "cpu"

    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  # ---------------- GloVe embeddings + Logistic Regression ----------
  glove:
    embeddings_path: "/path/to/Glove/glove300.txt"
    embedding_dim: 300

    lowercase: true
    remove_stopwords: true
    stopwords_lang: "english"

    # sentence embedding = sum(v_i) then L2-normalize
    pooling: "sum_l2norm"

    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  # ---------------- Gemma-2B embeddings + Logistic Regression -------
  gemma:
    model_name: "google/gemma-2b"
    pooling: "mean"            # sentence pooling over token embeddings

    max_seq_length: *max_seq_length
    padding: "longest"         # "longest" / "max_length"
    truncation: true

    device: "cuda"             # or "cpu"

    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  # ---------------- Cross-Validation common settings ----------------
  crossval:
    enabled: true
    n_splits: 5
    stratified: true
    shuffle: true
    random_state: *seed

    metrics_average: "macro"   # "macro" / "weighted"
