# =====================================================================
# config_text.yaml
# Central configuration for:
#   - ASR on NCMMSC audio (Whisper)
#   - Chinese text preprocessing & dataset merging
#   - Text feature extraction + classifiers + K-fold CV
#     (TF-IDF / BERT / GloVe / Gemma)
# =====================================================================

# ---------------------------------------------------------------------
# ASR (audio → transcript CSV) – used by asr_ncmmsc.py
# ---------------------------------------------------------------------
asr:
  # Root directory of NCMMSC audio (contains AD / HC / MCI subfolders)
  data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/merge"

  # Output ASR CSV (used later by text preprocessing)
  output_csv: "/content/ncmmsc_merged_asr_transcripts.csv"

  # Whisper model settings
  model_size: "large-v2"
  device: "cuda"            # "cuda" or "cpu"
  compute_type: "float16"   # e.g. "float16", "int8_float16", ...

  # Initial prompt given to Whisper (Chinese clinical context)
  initial_prompt: |-
    以下是一段中文口語錄音的逐字稿，內容為針對失智症、阿茲海默症、
    輕度認知障礙（MCI）與健康對照組（HC）的臨床訪談與語言測驗，
    包括圖畫描述任務、語意流暢度測驗以及日常生活相關問答。
    請以繁體中文完整逐字轉寫受試者與訪談者的口語內容，不要翻譯、
    不要潤飾或做任何摘要，也不要自行更改語序或補上沒聽到的字。
    請保留口語語氣詞與猶豫聲（例如：嗯、呃、啊、就是、然後）、
    重複、修正、語法不完整或中斷的句子，這些特徵可能與認知功能相關。
    遇到下列專有名詞或相關用語時，請盡量使用常見且一致的寫法：
    「失智症」、「阿茲海默症」、「阿茲海默病」、「輕度認知障礙」、
    「MCI」、「AD」、「HC」、「記憶力」、「認知功能」、
    「量表」、「測驗」、「醫師」、「受試者」、「照顧者」。
    對於 AD、MCI、HC 等縮寫，請以大寫英文字母保留。
    若有單字實在聽不清楚，請不要亂猜，可以以『[聽不清楚]』標示。

  # Decoding hyper-parameters for Whisper (all model-side settings live here)
  decode:
    language: "zh"
    task: "transcribe"
    beam_size: 10
    patience: 1.0
    length_penalty: 1.0
    temperature: 0.0
    compression_ratio_threshold: 2.4
    log_prob_threshold: -1.0
    no_speech_threshold: 0.6
    condition_on_previous_text: true
    vad_filter: false
    without_timestamps: true

# ---------------------------------------------------------------------
# Text preprocessing & dataset merging – used by preprocess_chinese.py
# ---------------------------------------------------------------------
text:
  # JSONL converted from ASR CSV for NCMMSC
  ncmmsc_jsonl: "/content/data_Chinese-NCMMSC2021_AD_Competition.jsonl"

  # Other Chinese dataset (already in JSONL)
  predictive_jsonl: "/content/Chinese-predictive_challenge_tsv2_output.jsonl"

  # (Optional) TAUKADIAL JSONL – can be omitted if not available
  # taukadial_jsonl: "/content/TAUKADIAL.jsonl"

  # Directory to store merged & preprocessed JSONL files
  output_dir: "/content/chinese_combined"

  # Name of merged JSONL combining all Chinese datasets
  combined_name: "Chinese_NCMMSC_iFlyTek.jsonl"

  # Filenames for final train/test splits (written into output_dir)
  train_jsonl: "train_chinese.jsonl"
  test_jsonl: "test_chinese.jsonl"

# ---------------------------------------------------------------------
#    Text features + classifiers (TF-IDF / BERT / GloVe / Gemma)
#    + K-fold cross-validation
# ---------------------------------------------------------------------
features:
  # Which text feature method to run in the current experiment:
  #   "tfidf" / "bert" / "glove" / "gemma"
  method: "bert"

  # ---------------- TF-IDF + Logistic Regression --------------------
  tfidf:
    vectorizer:
      lowercase: true
      max_features: 20000
      ngram_range: [1, 2]      # (1, 2) = unigram + bigram
      stop_words: null         # Chinese: keep null, or set your own list
      min_df: 1
      max_df: 1.0

    transformer:
      use_idf: true
      smooth_idf: true
      norm: "l2"
      sublinear_tf: false

    logreg:
      C: 1.0
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: 1000
      random_state: 42

  # ---------------- BERT embeddings + Logistic Regression -----------
  bert:
    # For Chinese experiments, use a Chinese or multilingual model
    # e.g. "bert-base-chinese" or "hfl/chinese-roberta-wwm-ext"
    model_name: "bert-base-chinese"
    pooling: "mean"            # "mean" / "cls" / "last4_mean"
    max_seq_length: 512

    device: "cuda"             # or "cpu"

    logreg:
      C: 1.0
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: 1000
      random_state: 42

  # ---------------- GloVe embeddings + Logistic Regression ----------
  # Mainly for English experiments (e.g. DementiaNet). Not used in the
  # Chinese experiments unless you provide a Chinese embedding file.
  glove:
    embeddings_path: "/path/to/Glove/glove300.txt"
    embedding_dim: 300

    lowercase: true
    remove_stopwords: true
    stopwords_lang: "english"

    pooling: "sum_l2norm"   # sum v_i then L2-normalize

    logreg:
      C: 1.0
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: 1000
      random_state: 42

  # ---------------- Gemma-2B embeddings + Logistic Regression -------
  gemma:
    model_name: "google/gemma-2b"
    pooling: "mean"          # sentence pooling over token embeddings

    max_seq_length: 512
    padding: "longest"       # "longest" / "max_length"
    truncation: true

    device: "cuda"           # or "cpu"

    logreg:
      C: 1.0
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: 1000
      random_state: 42

  # ---------------- Cross-Validation common settings ----------------
  crossval:
    enabled: true
    n_splits: 5
    stratified: true
    shuffle: true
    random_state: 42

    metrics_average: "macro"   # "macro" / "weighted"
