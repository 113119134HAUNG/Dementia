# =====================================================================
# config_text.yaml (single source of truth, paper-strict, converged)
# =====================================================================

common:
  seed: &seed 42

  split_test_size: &split_test_size 0.2
  length_std_k: &length_std_k 1.0
  min_per_class: &min_per_class 2

  logreg_C: &logreg_C 1.0
  logreg_max_iter: &logreg_max_iter 1000
  max_seq_length: &max_seq_length 512

asr:
  # choose ONE:
  data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/raw_vad"
  # data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/raw"
  # data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/merge"
  # data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/merge_vad"

  output_csv: "/content/ncmmsc_merged_asr_transcripts.csv"
  model_size: "large-v2"
  device: "cuda"
  compute_type: "float16"

  initial_prompt: |-
    以下是一段中文口語錄音的逐字稿，內容為針對失智症、阿茲海默症、
    輕度認知障礙（MCI）與健康對照組（HC）的臨床訪談與語言測驗，
    包括圖畫描述任務、語意流暢度測驗以及日常生活相關問答。
    請以繁體中文完整逐字轉寫受試者與訪談者的口語內容，不要翻譯、
    不要潤飾或做任何摘要，也不要自行更改語序或補上沒聽到的字。
    請保留口語語氣詞與猶豫聲（例如：嗯、呃、啊、就是、然後）、
    重複、修正、語法不完整或中斷的句子，這些特徵可能與認知功能相關。
    遇到下列專有名詞或相關用語時，請盡量使用常見且一致的寫法：
    「失智症」、「阿茲海默症」、「阿茲海默病」、「輕度認知障礙」、
    「MCI」、「AD」、「HC」、「記憶力」、「認知功能」、
    「量表」、「測驗」、「醫師」、「受試者」、「照顧者」。
    對於 AD、MCI、HC 等縮寫，請以大寫英文字母保留。
    若有單字實在聽不清楚，請不要亂猜，可以以『【聽不清楚】』標示。

  decode:
    language: "zh"
    task: "transcribe"
    beam_size: 10
    patience: 1.0
    length_penalty: 1.0
    temperature: 0.0
    compression_ratio_threshold: 2.4
    log_prob_threshold: -1.0
    no_speech_threshold: 0.6
    condition_on_previous_text: true
    vad_filter: false
    without_timestamps: true

predictive:
  meta_csv: "/content/NCMMSC2021_AD_Competition-dev/2_final_list_train.csv"
  egemaps_csv: "/content/NCMMSC2021_AD_Competition-dev/egemaps_final.csv"
  tsv_root: "/content/NCMMSC2021_AD_Competition-dev/tsv2"
  output_text_jsonl: &predictive_jsonl "/content/Chinese-predictive_challenge_tsv2_output.jsonl"
  output_egemaps_csv: "/content/predictive_egemaps_features.csv"
  dataset_name: "Chinese_predictive_challenge"

  keep_speakers: &pred_keep_speakers ["<A>", "<B>"]
  tsv:
    keep_speakers: *pred_keep_speakers
    drop_silence: true
    order_by: "no"

text:
  output_dir: "/content/chinese_combined"
  combined_name: "Chinese_Combined.jsonl"
  cleaned_jsonl: "/content/chinese_combined/cleaned.jsonl"

  ncmmsc_jsonl: &ncmmsc_jsonl "/content/chinese_combined/ncmmsc_from_asr.jsonl"

  corpora:
    - name: "NCMMSC2021_AD_Competition"
      path: *ncmmsc_jsonl
    - name: "Chinese_predictive_challenge"
      path: *predictive_jsonl
    - name: "TAUKADIAL"
      path: null

  target_datasets: ["NCMMSC2021_AD_Competition"]
  target_labels: ["AD", "HC"]

  balance: true
  subset_seed: *seed
  cap_per_class: 300

  label_map:
    NC: "HC"
    CN: "HC"
    CTRL: "HC"
    CONTROL: "HC"
    CONTROLS: "HC"
    HEALTHY: "HC"
    NORMAL: "HC"
    PROBABLEAD: "AD"
    POSSIBLEAD: "AD"
    MILDCOGNITIVEIMPAIRMENT: "MCI"

  language_filter:
    drop_languages: ["en"]

  # NEW: quality filter (after cleaning)
  quality_filter:
    enabled: true
    min_chars: 30
    min_han: 20
    max_unk_ratio: 0.30

  length_filter:
    enabled: true
    std_k: *length_std_k

features:
  tfidf:
    vectorizer:
      analyzer: "char"
      ngram_range: [2, 4]   # keep as list; code will convert to tuple
      lowercase: false
      max_features: 50000
      stop_words: null
      min_df: 1
      max_df: 1.0
    transformer:
      use_idf: true
      smooth_idf: true
      norm: "l2"
      sublinear_tf: false
    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  bert:
    model_name: "/content/models/bert-base-chinese"
    embedding_strategy: "mean"
    pooling: "mean"
    last_n_layers: 4
    max_seq_length: *max_seq_length
    device: "cuda"
    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  glove:
    embeddings_path: "/content/embeddings/cc.zh.300.vec"
    embedding_dim: 300
    tokenizer: "jieba"
    max_words: 50000
    lowercase: false
    remove_stopwords: false
    stopwords_lang: null
    pooling: "sum_l2norm"
    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  gemma:
    model_name: "/content/models/google__gemma-2b"
    pooling: "mean"
    max_seq_length: *max_seq_length
    device: "cuda"
    logreg:
      C: *logreg_C
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: *logreg_max_iter
      random_state: *seed

  crossval:
    enabled: true
    n_splits: 5
    shuffle: true
    random_state: *seed
    output_indices: "/content/chinese_combined/folds_indices.json"

    methods: ["tfidf", "bert", "glove", "gemma"]

    tfidf_fit_scope: "full"   # "full" | "train"

    batch_size: 8
    bert_batch_size: 8
    gemma_batch_size: 4

    metrics_average: "macro"
    zero_division: 0
    metrics_output: "/content/chinese_combined/cv_metrics.json"

    print_classification_report: true
    print_confusion_matrix: true
    print_method_summary: true
