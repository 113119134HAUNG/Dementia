# =====================================================================
# config_text.yaml
# Central configuration for:
#   - ASR on NCMMSC audio (Whisper)
#   - Chinese text preprocessing & dataset merging
#   - Text feature extraction + classifiers + K-fold CV (TF-IDF / BERT / GloVe / Gemma)
# =====================================================================

# ---------------------------------------------------------------------
# 1. ASR (audio → transcript CSV) – used by asr_ncmmsc.py
# ---------------------------------------------------------------------
asr:
  # Root directory of NCMMSC audio (contains AD / HC / MCI subfolders)
  data_root: "/content/NCMMSC2021_AD_Competition-dev/dataset/merge"

  # Output ASR CSV (used later by text preprocessing)
  output_csv: "/content/ncmmsc_merged_asr_transcripts.csv"

  # Whisper model settings
  model_size: "large-v2"
  device: "cuda"          # "cuda" or "cpu"
  compute_type: "float16" # e.g. "float16", "int8_float16", ...

  # Initial prompt given to Whisper (Chinese clinical context)
  initial_prompt: |-
    以下是一段中文口語錄音的逐字稿，內容為針對失智症、阿茲海默症、
    輕度認知障礙（MCI）與健康對照組（HC）的臨床訪談與語言測驗，
    包括圖畫描述任務、語意流暢度測驗以及日常生活相關問答。
    請以繁體中文完整逐字轉寫受試者與訪談者的口語內容，不要翻譯、
    不要潤飾或做任何摘要，也不要自行更改語序或補上沒聽到的字。
    請保留口語語氣詞與猶豫聲（例如：嗯、呃、啊、就是、然後）、
    重複、修正、語法不完整或中斷的句子，這些特徵可能與認知功能相關。
    遇到下列專有名詞或相關用語時，請盡量使用常見且一致的寫法：
    「失智症」、「阿茲海默症」、「阿茲海默病」、「輕度認知障礙」、
    「MCI」、「AD」、「HC」、「記憶力」、「認知功能」、
    「量表」、「測驗」、「醫師」、「受試者」、「照顧者」。
    對於 AD、MCI、HC 等縮寫，請以大寫英文字母保留。
    若有單字實在聽不清楚，請不要亂猜，可以以『[聽不清楚]』標示。

  # Decoding hyper-parameters for Whisper (all model-side settings live here)
  decode:
    language: "zh"
    task: "transcribe"
    beam_size: 10
    patience: 1.0
    length_penalty: 1.0
    temperature: 0.0
    compression_ratio_threshold: 2.4
    log_prob_threshold: -1.0
    no_speech_threshold: 0.6
    condition_on_previous_text: true
    vad_filter: false
    without_timestamps: true

# ---------------------------------------------------------------------
# 2. Text preprocessing & dataset merging – used by preprocess_chinese.py
# ---------------------------------------------------------------------
text:
  # JSONL converted from ASR CSV for NCMMSC
  ncmmsc_jsonl: "/content/data_Chinese-NCMMSC2021_AD_Competition.jsonl"

  # Other Chinese datasets (already in JSONL)
  predictive_jsonl: "/content/Chinese-predictive_challenge_tsv2_output.jsonl"

  # Directory to store merged & preprocessed JSONL files
  output_dir: "/content/chinese_combined"

  # Name of merged JSONL combining all Chinese datasets
  combined_name: "Chinese_NCMMSC_iFlyTek.jsonl"

  # Filenames for final train/test splits (written into output_dir)
  train_jsonl: "train_chinese.jsonl"
  test_jsonl: "test_chinese.jsonl"

# ---------------------------------------------------------------------
# 3. Text feature & classifier settings – TF-IDF / BERT / GloVe / Gemma
#    + K-fold cross-validation.  This block describes the *modeling*
#    experiment that goes into the paper.
# ---------------------------------------------------------------------
features:
  # Which text feature method to run in the current experiment:
  #   "tfidf" / "bert" / "glove" / "gemma"
  method: "bert"

  # ---------------- TF-IDF + Logistic Regression --------------------
  tfidf:
    # CountVectorizer settings
    vectorizer:
      lowercase: true
      max_features: 20000
      ngram_range: [1, 2]      # (1, 2) = unigram + bigram
      stop_words: null         # "english" / null / (你之後也可以自訂)
      min_df: 1
      max_df: 1.0

    # TfidfTransformer settings
    transformer:
      use_idf: true
      smooth_idf: true
      norm: "l2"
      sublinear_tf: false

    # Logistic Regression hyper-parameters
    logreg:
      C: 1.0
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: 1000
      random_state: 42

  # ---------------- BERT embeddings + Logistic Regression -----------
  bert:
    # HF model name
    model_name: "bert-base-uncased"   # or "bert-large-uncased"
    pooling: "mean"                   # "mean" / "cls" / "last4_mean"
    max_seq_length: 512

    # Device config for BERT encoding
    device: "cuda"                    # or "cpu"

    logreg:
      C: 1.0
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: 1000
      random_state: 42

  # ---------------- GloVe embeddings + Logistic Regression ----------
  glove:
    # Path to 300-dim GloVe vectors (you need to update this path)
    embeddings_path: "/path/to/Glove/glove300.txt"
    embedding_dim: 300

    lowercase: true
    remove_stopwords: true
    stopwords_lang: "english"

    # "sum_l2norm" = sum v_i then L2-normalize (what you’re doing now)
    pooling: "sum_l2norm"

    logreg:
      C: 1.0
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: 1000
      random_state: 42

  # ---------------- Gemma-2B embeddings + Logistic Regression -------
  gemma:
    # HF model name for Gemma
    model_name: "google/gemma-2b"

    # Sentence pooling strategy over token embeddings
    pooling: "mean"              # currently: last_hidden_state.mean(dim=1)

    # Tokenizer behavior
    max_seq_length: 512
    padding: "longest"           # "longest" / "max_length"
    truncation: true

    # Device config for Gemma encoding
    device: "cuda"               # or "cpu"

    logreg:
      C: 1.0
      class_weight: "balanced"
      solver: "lbfgs"
      max_iter: 1000
      random_state: 42

  # ---------------- Cross-Validation common settings ----------------
  crossval:
    enabled: true
    n_splits: 5
    stratified: true
    shuffle: true
    random_state: 42

    # How to average precision/recall/F1 across classes
    metrics_average: "macro"   # e.g. "macro" / "weighted"
